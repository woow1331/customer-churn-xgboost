# ğŸ“‰ Customer Churn Prediction Project (XGBoost + Feature Engineering)

## ğŸ“Œ Project Overview
This project predicts customer churn based on the **Telco Customer Churn Dataset**.  
Using a full ML pipeline with `XGBoost`, we handle class imbalance, evaluate model performance (AUC 0.82), and interpret predictions through feature importance.

> ğŸ¯ **Goal:** Help businesses identify customers likely to leave and take proactive retention actions.

---

## ğŸ“ Dataset

- ğŸ“ Source: [Kaggle - Telco Customer Churn](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)
- ğŸ“Š 7,000+ customer records
- ğŸ¯ Target variable: `Churn` (Yes/No)

---

## ğŸ”§ Preprocessing Workflow

1. Dropped `customerID` (non-informative)
2. Split columns into numerical & categorical
3. Used `ColumnTransformer`:
   - ğŸ”¢ Standardised numerical columns
   - ğŸ”¤ One-hot encoded categorical columns (`drop="first"`)

```python
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numerical_cols),
    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)
])
```

---

## âš–ï¸ Why Use `scale_pos_weight`?

The dataset is **imbalanced** â€” most customers do not churn.  
Without weighting, the model overpredicts "No Churn" and misses real churners.

We used:
```python
scale_pos_weight = 1294 / 467 â‰ˆ 2.77
```

This significantly **boosted recall for class 1**, improving overall effectiveness.

---

## ğŸ“Š Model Comparison

| Variant              | Precision (churn) | Recall (churn) | AUC  |
|----------------------|------------------|----------------|------|
| Without weighting    | 0.61             | 0.49           | 0.78 |
| **With weighting**   | **0.53**         | **0.67**       | **0.82** |

> âœ… Trade-off: Slight drop in precision, big gain in recall â€” a good trade for retention strategies.

---

## ğŸ¤– Model & Training Code

```python
pipeline_weighted = Pipeline([
    ('preprocessor', preprocessor),
    ('model', XGBClassifier(
        use_label_encoder=False,
        eval_metric='logloss',
        scale_pos_weight=2.77,
        random_state=42
    ))
])
pipeline_weighted.fit(X_train, y_train)
```

---

## ğŸ“ˆ Evaluation Results

- **Accuracy**: ~78%
- **Recall (churn)**: ~0.67 âœ…
- **Confusion Matrix**: Increased true positives for churners
- **AUC Score**: `0.82` â†’ strong overall classification performance

![ROC Curve](assets/roc.png)

---

## ğŸ” Feature Importance

Top influential features:
1. Contract_Two year
2. InternetService_Fiber optic
3. Contract_One year
4. InternetService_No

> ğŸ“Œ Long-term contracts significantly reduce churn risk.

![Feature Importance](assets/importance.png)

---

## ğŸ“Š External Dashboard

ğŸ”— [Interactive Tableau Dashboard](https://public.tableau.com/app/profile/yourname/viz/SuperstoreDashboard)

---

## ğŸ§  Conclusion

- Adjusting for class imbalance **improved churn recall by ~37%**
- Feature analysis reveals business drivers of churn
- Model is interpretable, actionable, and production-ready

---

## ğŸ§° Skills Demonstrated

- âœ… Data preprocessing (`StandardScaler`, `OneHotEncoder`)
- âœ… ML pipeline construction (`Pipeline`, `ColumnTransformer`)
- âœ… XGBoost modeling + tuning
- âœ… Model evaluation (precision, recall, ROC, AUC)
- âœ… Interpretability (feature importance)
- âœ… Data storytelling & presentation

---

> ğŸš€ Built to support churn reduction strategies in real-world scenarios.
